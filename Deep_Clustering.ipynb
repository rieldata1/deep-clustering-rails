{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM6gf4ijl2u/ZknYr5En6SG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rieldata1/deep-clustering-rails/blob/main/Deep_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurar GPU y activar high-RAM**"
      ],
      "metadata": {
        "id": "72GY_P1p9Wj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Drive\n",
        "from google import colab\n",
        "colab.drive.mount('/content/drive')\n",
        "\n",
        "# Configurar GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# Activar high-RAM\n",
        "import psutil\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "537a0zTL9qbT",
        "outputId": "06535ac5-f30f-40b8-ae3b-3ffc7f5329df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Tue Sep  2 20:12:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   51C    P0             28W /   72W |    1041MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Your runtime has 56.9 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep_Clustering: carga NPZ + AE + K-sweep + DEC (+ IDEC)**\n",
        "* Este notebook consumirá los NPZ generados por \"Scalograms\".\n",
        "* Optimizaciones: GPU, AMP (mixed precision), prefetch, pin_memory.\n"
      ],
      "metadata": {
        "id": "L_Fnr6U8_PhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Preeliminares**"
      ],
      "metadata": {
        "id": "zE-BJGvW_y6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5NHoTV16s0M",
        "outputId": "7f2531ef-9017-40e2-eb2f-a22b49f5e236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU detectada: NVIDIA L4\n",
            "Carpeta de ejecución: /content/drive/MyDrive/Deep_Cluster/experiments/run_20250902_201248\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# PARÁMETROS (ajustar a voluntad)\n",
        "# -----------------------------\n",
        "# Rutas (apunta al índice creado por \"Scalograms.ipynb\")\n",
        "BASE_DIR           = \"/content/drive/MyDrive/Deep_Cluster\"\n",
        "INDEX_CSV          = f\"{BASE_DIR}/meta/scalos_train_index.csv\"     # índice principal\n",
        "EXPERIMENTS_DIR    = f\"{BASE_DIR}/experiments\"                     # carpeta para guardar resultados (modelos, history)\n",
        "\n",
        "# Carga de datos\n",
        "PRELOAD_DATA       = True      # True: intenta precargar todos los NPZ en RAM si caben; False: lee por streaming\n",
        "PRELOAD_MAX_GB     = 4.0       # umbral aprox para decidir si precarga (ajústalo según tu Colab Pro)\n",
        "IMG_NORMALIZE_AGAIN= False     # renormalizar a [0,1] al vuelo (no debería hacer falta si Scalograms ya normalizó)\n",
        "\n",
        "# Sampler (diagnóstico): balancear por etiqueta simulada (si existe en NPZ)\n",
        "BALANCE_BY_LABEL   = False\n",
        "\n",
        "# Batching / DataLoader\n",
        "BATCH_AE           = 64        # batch para preentrenamiento del Autoencoder\n",
        "BATCH_EMB          = 128       # batch para extraer embeddings\n",
        "BATCH_DEC          = 64        # batch para DEC/IDEC\n",
        "NUM_WORKERS        = 2         # sube si tienes CPU libre; si da problemas, pon 0\n",
        "PIN_MEMORY         = True\n",
        "PERSISTENT_WORKERS = True\n",
        "\n",
        "# Modelo\n",
        "IMG_SIZE           = (256, 256) # se verificará contra el CSV; aquí sirve como referencia\n",
        "LATENT_DIM         = 128        # tamaño del embedding z\n",
        "BACKBONE           = \"base\"     # 'small' | 'base' | 'large' (tamaño de la CNN)\n",
        "DROPOUT_P          = 0.0\n",
        "\n",
        "# Entrenamiento AE (preentrenamiento)\n",
        "AE_EPOCHS          = 25\n",
        "AE_LR              = 1e-3\n",
        "AE_WD              = 1e-5\n",
        "AE_LOSS            = \"l1\"       # 'l1' o 'mse'\n",
        "USE_AMP            = True       # usar mixed precision en GPU (acelera / ahorra RAM)\n",
        "\n",
        "# Barrido de K (auto-K)\n",
        "K_MIN              = 2\n",
        "K_MAX              = 10\n",
        "K_FIXED            = None       # si quieres saltar el barrido, pon un entero (p. ej., 4)\n",
        "K_RANDOM_STATE     = 2025\n",
        "\n",
        "# DEC (clustering profundo)\n",
        "DEC_EPOCHS         = 40\n",
        "DEC_LR             = 1e-4\n",
        "DEC_WD             = 0.0\n",
        "DEC_UPDATE_INT     = 1          # actualizar distribución objetivo P cada N épocas\n",
        "DEC_TOL            = 1e-3       # criterio de estabilidad de asignaciones\n",
        "\n",
        "# IDEC (opcional: DEC + pérdida de reconstrucción)\n",
        "RUN_IDEC           = False\n",
        "IDEC_EPOCHS        = 40\n",
        "IDEC_LR            = 1e-4\n",
        "IDEC_WD            = 0.0\n",
        "IDEC_LAMBDA_REC    = 1e-2\n",
        "IDEC_UPDATE_INT    = 1\n",
        "IDEC_TOL           = 1e-3\n",
        "\n",
        "# Visualización / Evaluación\n",
        "RUN_TSNE_2D        = True\n",
        "TSNE_PERPLEXITY    = 30\n",
        "SEED               = 153\n",
        "\n",
        "# Guardado de artefactos (pesos, centros, history, config)\n",
        "SAVE_ARTIFACTS     = True\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# IMPORTS (solo librerías estándar de Colab)\n",
        "# -----------------------------\n",
        "import os, time, csv, math, gc, json, random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Runtime / Device (GPU + High-RAM)\n",
        "# -----------------------------\n",
        "def seed_everything(seed=SEED):\n",
        "    \"\"\"Fija semillas y activa pequeñas optimizaciones del backend.\"\"\"\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # Para CNNs suele acelerar (usa kernel heurístico por tamaño)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # Precisión alta para matmul en GPUs modernas\n",
        "    if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# Asegúrate de seleccionar GPU y High-RAM en: Runtime → Change runtime type\n",
        "seed_everything(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if DEVICE.type == \"cuda\":\n",
        "    print(\"✅ GPU detectada:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"⚠️  No se detectó GPU: ejecutará en CPU (más lento).\")\n",
        "\n",
        "# Crear carpetas de experimento\n",
        "os.makedirs(EXPERIMENTS_DIR, exist_ok=True)\n",
        "RUN_DIR = os.path.join(EXPERIMENTS_DIR, datetime.now().strftime(\"run_%Y%m%d_%H%M%S\"))\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"Carpeta de ejecución:\", RUN_DIR)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utils simples\n",
        "# -----------------------------\n",
        "def sizeof_gb(n_items, h, w, bytes_per=2):\n",
        "    \"\"\"Estimación de tamaño (GB) para n_items imágenes HxW en float16 (2 bytes por valor).\"\"\"\n",
        "    return (n_items * h * w * bytes_per) / (1024**3)\n",
        "\n",
        "def count_params(m):\n",
        "    \"\"\"Cuenta parámetros entrenables de un modelo.\"\"\"\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) DATASET**"
      ],
      "metadata": {
        "id": "VmVHfJQDDuAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1) DATASET (100% no supervisado): descubre .npz y carga imágenes\n",
        "#     - No lee labels\n",
        "#     - No usa index.csv\n",
        "#     - Precarga opcional si cabe en RAM\n",
        "# ============================================================\n",
        "\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# --------- Descubrir archivos .npz del directorio de datos ----------\n",
        "# Si no definiste DATA_DIR en la Celda 0, lo inferimos desde BASE_DIR\n",
        "try:\n",
        "    DATA_DIR\n",
        "except NameError:\n",
        "    DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "\n",
        "NPZ_GLOB = os.path.join(DATA_DIR, \"*.npz\")\n",
        "npz_files = sorted(glob.glob(NPZ_GLOB))\n",
        "if not npz_files:\n",
        "    raise FileNotFoundError(f\"No se encontraron .npz en: {NPZ_GLOB}\\n\"\n",
        "                            f\"Verifica que ya corriste 'Scalograms' y que BASE_DIR/ DATA_DIR son correctos.\")\n",
        "\n",
        "print(f\"[SCAN] Archivos .npz encontrados: {len(npz_files)}\")\n",
        "for p in npz_files[:3]:\n",
        "    print(\"   •\", p)\n",
        "if len(npz_files) > 3:\n",
        "    print(\"   • ...\")\n",
        "\n",
        "\n",
        "# --------- Manifest: conteo de muestras por .npz y tamaño HxW ----------\n",
        "class NPZManifest:\n",
        "    \"\"\"Escanea los .npz y guarda:\n",
        "       - files: lista de rutas\n",
        "       - counts: # de escalogramas por archivo\n",
        "       - cumcounts: prefix sum para mapear índice global → (archivo, índice local)\n",
        "       - img_h, img_w: tamaño de los escalogramas (se valida consistencia)\n",
        "    \"\"\"\n",
        "    def __init__(self, files):\n",
        "        self.files = list(files)\n",
        "        self.counts = []\n",
        "        self.cumcounts = [0]\n",
        "        self.img_h = None\n",
        "        self.img_w = None\n",
        "\n",
        "        for p in self.files:\n",
        "            d = np.load(p, allow_pickle=True)\n",
        "            X = d[\"X\"]  # (B, H, W) en float16\n",
        "            B, H, W = X.shape\n",
        "            self.counts.append(B)\n",
        "            self.cumcounts.append(self.cumcounts[-1] + B)\n",
        "            if self.img_h is None:\n",
        "                self.img_h, self.img_w = int(H), int(W)\n",
        "            else:\n",
        "                if (self.img_h, self.img_w) != (int(H), int(W)):\n",
        "                    raise ValueError(f\"Tamaños inconsistentes: {p} tiene {(H, W)} pero ya teníamos {(self.img_h, self.img_w)}\")\n",
        "\n",
        "        self.total = self.cumcounts[-1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total\n",
        "\n",
        "    def locate(self, global_idx):\n",
        "        \"\"\"Devuelve (file_path, local_idx) para un índice global.\"\"\"\n",
        "        # búsqueda binaria simple sobre cumcounts\n",
        "        lo, hi = 0, len(self.cumcounts)-1\n",
        "        while lo < hi:\n",
        "            mid = (lo + hi) // 2\n",
        "            if self.cumcounts[mid+1] <= global_idx:\n",
        "                lo = mid + 1\n",
        "            elif self.cumcounts[mid] > global_idx:\n",
        "                hi = mid - 1\n",
        "            else:\n",
        "                lo = mid\n",
        "                break\n",
        "        file_idx = lo\n",
        "        local_idx = global_idx - self.cumcounts[file_idx]\n",
        "        return self.files[file_idx], int(local_idx)\n",
        "\n",
        "manifest = NPZManifest(npz_files)\n",
        "print(f\"[MANIFEST] Total ventanas: {len(manifest)} | HxW={manifest.img_h}x{manifest.img_w}\")\n",
        "\n",
        "\n",
        "# --------- Estimar si conviene precargar a RAM ----------\n",
        "est_gb = sizeof_gb(len(manifest), manifest.img_h, manifest.img_w, bytes_per=2)  # float16 ~ 2 bytes\n",
        "do_preload = PRELOAD_DATA and (est_gb <= PRELOAD_MAX_GB + 1e-9)\n",
        "print(f\"[MEM] Estimado ~{est_gb:.2f} GB | Preload={do_preload} (umbral={PRELOAD_MAX_GB} GB)\")\n",
        "\n",
        "# --------- Dataset no supervisado ----------\n",
        "class ScalogramUnsupervisedDS(Dataset):\n",
        "    \"\"\"\n",
        "    Devuelve (x, '_'):\n",
        "      - x: tensor float32 (1, H, W) en [0,1]\n",
        "      - '_' es un placeholder (no usamos etiquetas)\n",
        "    \"\"\"\n",
        "    def __init__(self, manifest: NPZManifest, preload: bool = False, normalize_again: bool = IMG_NORMALIZE_AGAIN):\n",
        "        self.manifest = manifest\n",
        "        self.normalize_again = normalize_again\n",
        "        self.preload = preload\n",
        "        self.cache = {}  # path -> array X (B,H,W) en float16\n",
        "        if self.preload:\n",
        "            print(\"[LOAD] Precargando todos los .npz en RAM…\")\n",
        "            for p in tqdm(self.manifest.files, desc=\"Precarga\"):\n",
        "                X = np.load(p, allow_pickle=True)[\"X\"]  # (B,H,W) float16\n",
        "                self.cache[p] = X  # guardamos como float16 para ahorrar memoria\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.manifest)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        p, j = self.manifest.locate(i)\n",
        "        if p in self.cache:\n",
        "            Xj = self.cache[p][j]\n",
        "        else:\n",
        "            d = np.load(p, allow_pickle=True)\n",
        "            Xj = d[\"X\"][j]  # (H,W) float16\n",
        "\n",
        "        x = torch.from_numpy(np.asarray(Xj, dtype=np.float32)).unsqueeze(0)  # (1,H,W) float32\n",
        "\n",
        "        if self.normalize_again:\n",
        "            mn, mx = x.min(), x.max()\n",
        "            x = (x - mn) / (mx - mn + 1e-12)\n",
        "            x.clamp_(0.0, 1.0)\n",
        "\n",
        "        return x, \"_\"   # placeholder para mantener interfaz\n",
        "\n",
        "\n",
        "# --------- Instanciar dataset y splits (80/20) ----------\n",
        "ds_all = ScalogramUnsupervisedDS(manifest, preload=do_preload, normalize_again=IMG_NORMALIZE_AGAIN)\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "perm = rng.permutation(len(ds_all))\n",
        "n_tr = int(0.8 * len(ds_all))\n",
        "idx_tr, idx_va = perm[:n_tr], perm[n_tr:]\n",
        "\n",
        "ds_train = Subset(ds_all, idx_tr)\n",
        "ds_val   = Subset(ds_all, idx_va)\n",
        "\n",
        "# --------- DataLoaders ----------\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=BATCH_AE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
        "    persistent_workers=(NUM_WORKERS>0 and PERSISTENT_WORKERS),\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=BATCH_AE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
        "    persistent_workers=(NUM_WORKERS>0 and PERSISTENT_WORKERS),\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "def make_all_loader(ds, bs=BATCH_EMB):\n",
        "    return DataLoader(\n",
        "        ds, batch_size=bs, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
        "        persistent_workers=(NUM_WORKERS>0 and PERSISTENT_WORKERS),\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "dl_all = make_all_loader(ds_all, bs=BATCH_EMB)\n",
        "\n",
        "print(f\"[DATA] Train={len(ds_train)} | Val={len(ds_val)} | Total={len(ds_all)} | HxW={manifest.img_h}x{manifest.img_w}\")\n",
        "\n",
        "# --------- Rejilla rápida de muestras (opcional) ----------\n",
        "def show_grid_samples(dataset, n=12, rows=3, cols=4, title=\"Muestras del dataset (sin etiquetas)\"):\n",
        "    n = min(n, len(dataset), rows*cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(1.8*cols, 1.6*rows), sharex=True, sharey=True)\n",
        "    axes = np.array(axes).reshape(-1)\n",
        "    idxs = rng.choice(len(dataset), size=n, replace=False)\n",
        "    for k, i in enumerate(idxs):\n",
        "        x, _ = dataset[i]\n",
        "        axes[k].imshow(x.squeeze(0).numpy(), origin=\"lower\", aspect=\"auto\", cmap=\"turbo\")\n",
        "        axes[k].set_xticks([]); axes[k].set_yticks([])\n",
        "    for a in axes[n:]: a.axis(\"off\")\n",
        "    fig.suptitle(title); plt.tight_layout(); plt.show()\n",
        "\n",
        "# Descomenta si quieres visualizar una muestra\n",
        "# show_grid_samples(ds_all, n=12, rows=3, cols=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "dCzoIR6ZDtSR",
        "outputId": "2c902981-5411-4e00-ad16-6569982495e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SCAN] Archivos .npz encontrados: 29\n",
            "   • /content/drive/MyDrive/Deep_Cluster/data/scalos_train_000.npz\n",
            "   • /content/drive/MyDrive/Deep_Cluster/data/scalos_train_001.npz\n",
            "   • /content/drive/MyDrive/Deep_Cluster/data/scalos_train_002.npz\n",
            "   • ...\n",
            "[MANIFEST] Total ventanas: 1800 | HxW=256x256\n",
            "[MEM] Estimado ~0.22 GB | Preload=True (umbral=4.0 GB)\n",
            "[LOAD] Precargando todos los .npz en RAM…\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tqdm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2891478982.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;31m# --------- Instanciar dataset y splits (80/20) ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mds_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScalogramUnsupervisedDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanifest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_preload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_again\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_NORMALIZE_AGAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2891478982.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, manifest, preload, normalize_again)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[LOAD] Precargando todos los .npz en RAM…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Precarga\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (B,H,W) float16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m  \u001b[0;31m# guardamos como float16 para ahorrar memoria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SHv6C2wgDtAq"
      }
    }
  ]
}